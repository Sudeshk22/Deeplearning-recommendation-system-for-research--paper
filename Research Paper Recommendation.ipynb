{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9598d72f",
   "metadata": {},
   "source": [
    "\n",
    "# 1) Research Paper Recommendation for reading: using sentence transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "685a8db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers \n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a27ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data = pd.read_csv(\"arxiv_data_210930-054931.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d5b8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Multi-Level Attention Pooling for Graph Neural...</td>\n",
       "      <td>Graph neural networks (GNNs) have been widely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['cs.LG', 'cs.AI']</td>\n",
       "      <td>Decision Forests vs. Deep Networks: Conceptual...</td>\n",
       "      <td>Deep networks and decision forests (such as ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['cs.LG', 'cs.CR', 'stat.ML']</td>\n",
       "      <td>Power up! Robust Graph Convolutional Network v...</td>\n",
       "      <td>Graph convolutional networks (GCNs) are powerf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['cs.LG', 'cs.CR']</td>\n",
       "      <td>Releasing Graph Neural Networks with Different...</td>\n",
       "      <td>With the increasing popularity of Graph Neural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Recurrence-Aware Long-Term Cognitive Network f...</td>\n",
       "      <td>Machine learning solutions for pattern classif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           terms  \\\n",
       "0                      ['cs.LG']   \n",
       "1             ['cs.LG', 'cs.AI']   \n",
       "2  ['cs.LG', 'cs.CR', 'stat.ML']   \n",
       "3             ['cs.LG', 'cs.CR']   \n",
       "4                      ['cs.LG']   \n",
       "\n",
       "                                              titles  \\\n",
       "0  Multi-Level Attention Pooling for Graph Neural...   \n",
       "1  Decision Forests vs. Deep Networks: Conceptual...   \n",
       "2  Power up! Robust Graph Convolutional Network v...   \n",
       "3  Releasing Graph Neural Networks with Different...   \n",
       "4  Recurrence-Aware Long-Term Cognitive Network f...   \n",
       "\n",
       "                                           abstracts  \n",
       "0  Graph neural networks (GNNs) have been widely ...  \n",
       "1  Deep networks and decision forests (such as ra...  \n",
       "2  Graph convolutional networks (GCNs) are powerf...  \n",
       "3  With the increasing popularity of Graph Neural...  \n",
       "4  Machine learning solutions for pattern classif...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## checking the head of the dataset\n",
    "arxiv_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ec06e",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "188de23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56181, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa9e2647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "terms        0\n",
       "titles       0\n",
       "abstracts    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e04a62ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15054"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee3af998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:  ['cs.LG' 'cs.AI' 'cs.CR' ... 'D.1.3; G.4; I.2.8; I.2.11; I.5.3; J.3'\n",
      " '68T07, 68T45, 68T10, 68T50, 68U35' 'I.2.0; G.3']\n",
      "length: 1177\n"
     ]
    }
   ],
   "source": [
    "## getting the unique labels \n",
    "labels_column = arxiv_data['terms'].apply(literal_eval)\n",
    "labels = labels_column.explode().unique()\n",
    "print(\"labels: \", labels)\n",
    "print(\"length:\", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fa3f370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 41105 rows in the deduplicated dataset.\n",
      "2503\n",
      "3401\n"
     ]
    }
   ],
   "source": [
    "# remove duplicate entries based on the \"titles\" (terms) column\n",
    "# This filters the DataFrame, keeping only the rows where the titles are not duplicated.\n",
    "arxiv_data = arxiv_data[~arxiv_data['titles'].duplicated()]\n",
    "print(f\"There are {len(arxiv_data)} rows in the deduplicated dataset.\")\n",
    "# There are some terms with occurrence as low as 1.\n",
    "print(sum(arxiv_data['terms'].value_counts()==1))\n",
    "# how many unique terms\n",
    "print(arxiv_data['terms'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e894966d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38602, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the rare terms. (it keeps only those rows where the \"terms\" value occurs more than once in the original DataFrame.)\n",
    "arxiv_data_filtered = arxiv_data.groupby('terms').filter(lambda x: len(x) > 1)\n",
    "arxiv_data_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1b9e439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['cs.LG']), list(['cs.LG', 'cs.AI']),\n",
       "       list(['cs.LG', 'cs.CR', 'stat.ML'])], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# It evaluates the given string containing a Python literal or container display (e.g., a list or dictionary) and returns the corresponding Python object.\n",
    "arxiv_data_filtered['terms'] = arxiv_data_filtered['terms'].apply(lambda x: literal_eval(x))\n",
    "arxiv_data_filtered['terms'].values[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2a40d4",
   "metadata": {},
   "source": [
    "# train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d6c9de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 34741\n",
      "Number of rows in validation set: 1930\n",
      "Number of rows in test set: 1931\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_split = 0.1\n",
    "\n",
    "# Initial train and test split.\n",
    "# The stratify parameter ensures that the splitting is done in a way that preserves the same distribution of labels (terms) in both the training and test sets.\n",
    "train_df, test_df = train_test_split(arxiv_data_filtered,test_size=test_split,stratify=arxiv_data_filtered[\"terms\"].values,)\n",
    "\n",
    "# Splitting the test set further into validation\n",
    "# and new test sets.\n",
    "val_df = test_df.sample(frac=0.5)\n",
    "test_df.drop(val_df.index, inplace=True)\n",
    "\n",
    "print(f\"Number of rows in training set: {len(train_df)}\")\n",
    "print(f\"Number of rows in validation set: {len(val_df)}\")\n",
    "print(f\"Number of rows in test set: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e61a8e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "\n",
      "['[UNK]', 'cs.CV', 'cs.LG', 'stat.ML', 'cs.AI', 'eess.IV', 'cs.RO', 'cs.CL', 'cs.NE', 'cs.GR', 'cs.CR', 'math.OC', 'eess.SP', 'cs.SI', 'cs.MM', 'cs.SY', 'cs.IR', 'eess.SY', 'cs.MA', 'cs.HC', 'math.IT', 'cs.IT', 'cs.DC', 'stat.AP', 'cs.CY', 'stat.ME', 'stat.TH', 'math.ST', 'eess.AS', 'cs.DS', 'cs.SD', 'q-bio.QM', 'q-bio.NC', 'cs.CG', 'stat.CO', 'cs.GT', 'cs.NI', 'math.NA', 'cs.SE', 'cs.NA', 'I.2.6', 'physics.chem-ph', 'cs.DB', 'physics.comp-ph', 'cond-mat.dis-nn', 'q-bio.BM', 'math.PR', 'cs.PL', 'cs.LO', '68T45', 'cs.AR', 'physics.data-an', 'quant-ph', 'I.2.10', 'cs.CE', 'cond-mat.stat-mech', 'q-fin.ST', 'I.4.6', 'physics.ao-ph', 'math.DS', 'cs.CC', '68T05', 'physics.soc-ph', 'physics.med-ph', 'cs.PF', 'q-bio.GN', 'econ.EM', 'cs.DM', 'I.4.8', 'astro-ph.IM', 'physics.flu-dyn', 'math.AT', 'hep-ex', 'cs.FL', 'I.4', '68U10', 'q-fin.TR', 'physics.geo-ph', 'I.5.4', 'I.2', 'cond-mat.mtrl-sci', 'I.4.9', '68T10', 'physics.optics', 'I.4; I.5', '68T07', 'q-fin.CP', 'math.CO', 'math.AP', 'I.2.6; I.2.8', '65D19', 'q-bio.PE', 'physics.app-ph', 'nlin.CD', 'cs.MS', 'I.4.5', 'I.2.6; I.5.1', 'I.2.10; I.4; I.5', 'I.2.0; I.2.6', '68U01', '68T01', 'q-fin.GN', 'hep-ph', 'cs.SC', 'cs.ET', 'K.3.2', 'I.2.8', '68T30', 'q-fin.EC', 'q-bio.MN', 'econ.GN', 'I.4.9; I.5.4', 'I.4.0', 'I.2; I.5', 'I.2; I.4; I.5', 'I.2.6; I.2.7', 'I.2.10; I.4.8', '68T99', '68Q32', '68', '62H30', 'q-fin.RM', 'q-fin.PM', 'q-bio.TO', 'q-bio.OT', 'physics.plasm-ph', 'physics.class-ph', 'physics.bio-ph', 'nlin.AO', 'math.SP', 'math.MP', 'math.LO', 'math.FA', 'math-ph', 'cs.DL', 'cond-mat.soft', 'I.5.2', 'I.4.6; I.4.8', 'I.4.4', 'I.4.3', 'I.4.1', 'I.3.7', 'I.2; J.2', 'I.2; I.2.6; I.2.7', 'I.2.7', 'I.2.6; I.5.4', 'I.2.6; I.2.9', 'I.2.6; I.2.7; H.3.1; H.3.3', 'I.2.6; I.2.10', 'I.2.6, I.5.4', 'I.2.1; J.3', 'I.2.10; I.5.1; I.4.8', 'I.2.10; I.4.8; I.5.4', 'I.2.10; I.2.6', 'I.2.1', 'H.3.1; I.2.6; I.2.7', 'H.3.1; H.3.3; I.2.6; I.2.7', 'G.3', 'F.2.2; I.2.7', 'E.5; E.4; E.2; H.1.1; F.1.1; F.1.3', '68Txx', '62H99', '62H35', '60L10, 60L20', '14J60 (Primary) 14F05, 14J26 (Secondary)']\n"
     ]
    }
   ],
   "source": [
    "# creates a TensorFlow RaggedTensor (terms) from the values in the \"terms\" column of the train_df DataFrame. A RaggedTensor is a tensor with non-uniform shapes\n",
    "terms = tf.ragged.constant(train_df['terms'].values)\n",
    "# This line creates a StringLookup layer in TensorFlow. The purpose of this layer is to map strings to integer indices and vice versa. The output_mode=\"multi_hot\" indicates that the layer will output a multi-hot encoded representation of the input strings.\n",
    "lookup = tf.keras.layers.StringLookup(output_mode='multi_hot')\n",
    "# This step adapts the StringLookup layer to the unique values in the \"terms\" column, building the vocabulary.\n",
    "lookup.adapt(terms)\n",
    "# retrieve vocabulary\n",
    "vocab = lookup.get_vocabulary()\n",
    "\n",
    "print(\"Vocabulary:\\n\")\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ed7a97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: ['cs.LG', 'stat.ML']\n",
      "Label-binarized representation: [[0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "sample_label = train_df[\"terms\"].iloc[0]\n",
    "print(f\"Original label: {sample_label}\")\n",
    "\n",
    "label_binarized = lookup([sample_label])\n",
    "print(f\"Label-binarized representation: {label_binarized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "001f141a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn summary, the make_dataset function is designed to create a \\ndataset suitable for training a model. It takes a dataframe as input, \\nassumes it has \"abstracts\" and \"terms\" columns, and creates a dataset of \\nbatches where each batch consists of abstract \\nsequences and their corresponding binarized label sequences. \\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# following lines::\n",
    "# which is used for automatic adjustment of resource usage by TensorFlow's data loading pipeline.\n",
    "\n",
    "#max_seqlen: Maximum sequence length. It indicates the maximum length allowed for sequences.\n",
    "max_seqlen = 150\n",
    "#batch_size: Batch size. It specifies the number of samples to use in each iteration.\n",
    "batch_size = 128\n",
    "#padding_token: A token used for padding sequences.\n",
    "padding_token = \"<pad>\"\n",
    "#auto = tf.data.AUTOTUNE: auto is assigned the value tf.data.AUTOTUNE,\n",
    "auto = tf.data.AUTOTUNE\n",
    "def make_dataset(dataframe, is_train=True):\n",
    "    # creating sequences of labesls\n",
    "    labels = tf.ragged.constant(dataframe[\"terms\"].values)\n",
    "    #This line uses the previously defined lookup layer to convert the ragged tensor of labels into a binarized representation. The resulting label_binarized is a NumPy array.\n",
    "    label_binarized = lookup(labels).numpy()\n",
    "    # creating sequences of text.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataframe[\"abstracts\"].values, label_binarized))\n",
    "    # shuffling data basis on condition\n",
    "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "\"\"\"\n",
    "In summary, the make_dataset function is designed to create a \n",
    "dataset suitable for training a model. It takes a dataframe as input, \n",
    "assumes it has \"abstracts\" and \"terms\" columns, and creates a dataset of \n",
    "batches where each batch consists of abstract \n",
    "sequences and their corresponding binarized label sequences. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bed919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train_df, is_train=True)\n",
    "validation_dataset = make_dataset(val_df, is_train=False)\n",
    "test_dataset = make_dataset(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da1fc3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract: b'Person re-identification (Re-ID) aims to match person images across\\nnon-overlapping camera views. The majority of Re-ID methods focus on\\nsmall-scale surveillance systems in which each pedestrian is captured in\\ndifferent camera views of adjacent scenes. However, in large-scale surveillance\\nsystems that cover larger areas, it is required to track a pedestrian of\\ninterest across distant scenes (e.g., a criminal suspect escapes from one city\\nto another). Since most pedestrians appear in limited local areas, it is\\ndifficult to collect training data with cross-camera pairs of the same person.\\nIn this work, we study intra-camera supervised person re-identification across\\ndistant scenes (ICS-DS Re-ID), which uses cross-camera unpaired data with\\nintra-camera identity labels for training. It is challenging as cross-camera\\npaired data plays a crucial role for learning camera-invariant features in most\\nexisting Re-ID methods. To learn camera-invariant representation from\\ncross-camera unpaired training data, we propose a cross-camera feature\\nprediction method to mine cross-camera self supervision information from\\ncamera-specific feature distribution by transforming fake cross-camera positive\\nfeature pairs and minimize the distances of the fake pairs. Furthermore, we\\nautomatically localize and extract local-level feature by a transformer. Joint\\nlearning of global-level and local-level features forms a global-local\\ncross-camera feature prediction scheme for mining fine-grained cross-camera\\nself supervision information. Finally, cross-camera self supervision and\\nintra-camera supervision are aggregated in a framework. The experiments are\\nconducted in the ICS-DS setting on Market-SCT, Duke-SCT and MSMT17-SCT\\ndatasets. The evaluation results demonstrate the superiority of our method,\\nwhich gains significant improvements of 15.4 Rank-1 and 22.3 mAP on Market-SCT\\nas compared to the second best method.'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'invert_multi_hot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5396\\2130985761.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Abstract: {text}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Label(s): {invert_multi_hot(label[0])}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'invert_multi_hot' is not defined"
     ]
    }
   ],
   "source": [
    "# This code snippet is iterating through batches of the training dataset and printing the abstract text along with the corresponding labels.\n",
    "text_batch, label_batch = next(iter(train_dataset))\n",
    "for i, text in enumerate(text_batch[:5]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"Abstract: {text}\")\n",
    "    print(f\"Label(s): {invert_multi_hot(label[0])}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e71c27d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158850\n"
     ]
    }
   ],
   "source": [
    "# This code calculates the size of the vocabulary in the \"abstracts\" column of the train_df DataFrame.\n",
    "\n",
    "# Creating vocabulary with uniques words\n",
    "vocabulary = set()\n",
    "train_df[\"abstracts\"].str.lower().str.split().apply(vocabulary.update)\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c82d05",
   "metadata": {},
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e0a725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes a TextVectorization layer\n",
    "text_vectorizer = layers.TextVectorization(max_tokens=vocabulary_size,ngrams=2,output_mode=\"tf_idf\")\n",
    "# `TextVectorization` layer needs to be adapted as per the vocabulary from our\n",
    "# training set.\n",
    "text_vectorizer.adapt(train_dataset.map(lambda text, label: text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae70355",
   "metadata": {},
   "source": [
    "# Recommendation System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89bbb266",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data.drop(columns = [\"terms\",\"abstracts\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5593de0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data.drop_duplicates(inplace= True)\n",
    "arxiv_data.reset_index(drop= True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcf03e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Power up! Robust Graph Convolutional Network via Graph Powering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Releasing Graph Neural Networks with Differential Privacy Guarantees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41100</th>\n",
       "      <td>An experimental study of graph-based semi-supervised classification with additional node information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41101</th>\n",
       "      <td>Bayesian Differential Privacy through Posterior Sampling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41102</th>\n",
       "      <td>Mining Spatio-temporal Data on Industrialization from Historical Registries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41103</th>\n",
       "      <td>Wav2Letter: an End-to-End ConvNet-based Speech Recognition System</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41104</th>\n",
       "      <td>Generalized Low Rank Models</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41105 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 titles\n",
       "0      Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities\n",
       "1           Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes\n",
       "2                                                       Power up! Robust Graph Convolutional Network via Graph Powering\n",
       "3                                                  Releasing Graph Neural Networks with Differential Privacy Guarantees\n",
       "4                                   Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification\n",
       "...                                                                                                                 ...\n",
       "41100              An experimental study of graph-based semi-supervised classification with additional node information\n",
       "41101                                                          Bayesian Differential Privacy through Posterior Sampling\n",
       "41102                                       Mining Spatio-temporal Data on Industrialization from Historical Registries\n",
       "41103                                                 Wav2Letter: an End-to-End ConvNet-based Speech Recognition System\n",
       "41104                                                                                       Generalized Low Rank Models\n",
       "\n",
       "[41105 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "arxiv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd379a6",
   "metadata": {},
   "source": [
    "# Sentence Transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "319af1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e7536a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e55870306e946749beec1ca1f8d4d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudesh yadav\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sudesh yadav\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f038033daf4b4b8c0898f53fae3cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de309abafb774aa09df5c7b833aea509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011859872e2545b68d45b45c3bf2edef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce01a78dc3843d194b47240cdff7149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f12797412d42ca829705eff0eb3ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90306fe6bea04fd88611bbf12bd7d84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d0e4088c514191a6c7499aafd51e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479abce055564669bc00151a98220e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2453edca725d4a4a91a9b4efc04a3548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf13836ca03a43b580acfb2eb6f095e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe embeddings can be used for various natural language processing (NLP) tasks, \\nsuch as similarity search, clustering\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This imports the SentenceTransformer class from the Sentence Transformers library.\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "# we load all-MiniLM-L6-v2, which is a MiniLM model fine tuned on a large dataset of over \n",
    "# 1 billion training pairs.\n",
    "#This initializes the 'all-MiniLM-L6-v2' model from Sentence Transformers. \n",
    "# This model is capable of encoding sentences into fixed-size vectors (embeddings).\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#Our sentences we like to encode\n",
    "sentences = arxiv_data['titles']\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "\"\"\"\n",
    "The embeddings can be used for various natural language processing (NLP) tasks, \n",
    "such as similarity search, clustering\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a382c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06643412, -0.04954597,  0.06388088, ...,  0.00106303,\n",
       "        -0.1215638 , -0.06962783],\n",
       "       [ 0.09212257, -0.07606944,  0.06572863, ..., -0.08565164,\n",
       "        -0.09266546,  0.00725291],\n",
       "       [-0.08162688,  0.02428937,  0.01888741, ...,  0.00806161,\n",
       "        -0.05129534, -0.05874001],\n",
       "       ...,\n",
       "       [ 0.01227978, -0.08568835, -0.02782774, ..., -0.05257973,\n",
       "        -0.10806682,  0.07843313],\n",
       "       [-0.07258195, -0.12690923, -0.0053555 , ...,  0.03597708,\n",
       "        -0.03986151, -0.05971032],\n",
       "       [ 0.00768869, -0.10124185,  0.08909854, ..., -0.08199864,\n",
       "        -0.05649742,  0.09007055]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5bc2e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\sudesh yadav\\anaconda3\\lib\\site-packages (23.2.1)\n",
      "Collecting pip\n",
      "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/8a/6a/19e9fe04fca059ccf770861c7d5721ab4c2aebc539889e97c7977528a53b/pip-24.0-py3-none-any.whl.metadata\n",
      "  Downloading pip-24.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 2.1/2.1 MB 988.2 kB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.2.1\n",
      "    Uninstalling pip-23.2.1:\n",
      "      Successfully uninstalled pip-23.2.1\n",
      "Successfully installed pip-24.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "938f319b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Power up! Robust Graph Convolutional Network via Graph Powering\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Releasing Graph Neural Networks with Differential Privacy Guarantees\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Lifelong Graph Learning\n",
      "Embedding length: 384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "#This loop iterates over pairs of sentences and their corresponding embeddings. \n",
    "#zip is used to iterate over both lists simultaneously.\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding length:\", len(embedding)) # list of floats\n",
    "    print(\"\")\n",
    "    # Breaks out of the loop after printing information for the first 5 sentences.\n",
    "    if c >=5:\n",
    "        break\n",
    "    c +=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7c852",
   "metadata": {},
   "source": [
    "# save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bfea9ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "# Saving sentences and corresponding embeddings\n",
    "with open('models/embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "with open('models/sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(sentences, f)\n",
    "    \n",
    "with open('models/rec_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b43bf0b",
   "metadata": {},
   "source": [
    "# Recommendation for similar papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05a884d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load save files\n",
    "embeddings = pickle.load(open('models/embeddings.pkl','rb'))\n",
    "sentences = pickle.load(open('models/sentences.pkl','rb'))\n",
    "rec_model = pickle.load(open('models/rec_model.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3dfdfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "def recommendation(input_paper):\n",
    "    # Calculate cosine similarity scores between the embeddings of input_paper and all papers in the dataset.\n",
    "    cosine_scores = util.cos_sim(embeddings, rec_model.encode(input_paper))\n",
    "    \n",
    "    # Get the indices of the top-k most similar papers based on cosine similarity.\n",
    "    top_similar_papers = torch.topk(cosine_scores, dim=0, k=5, sorted=True)\n",
    "                                 \n",
    "    # Retrieve the titles of the top similar papers.\n",
    "    papers_list = []\n",
    "    for i in top_similar_papers.indices:\n",
    "        papers_list.append(sentences[i.item()])\n",
    "    \n",
    "    return papers_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36454ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the title of any paper you likeAttention is All you Need\n",
      "We recommend to read this paper............\n",
      "=============================================\n",
      "Attention that does not Explain Away\n",
      "Area Attention\n",
      "Pay Attention when Required\n",
      "Long Short-Term Attention\n",
      "Attention as Activation\n"
     ]
    }
   ],
   "source": [
    "# exampel usage 1: (use this paper as input (Attention is All you Need))\n",
    "input_paper = input(\"Enter the title of any paper you like\")\n",
    "recommend_papers = recommendation(input_paper)\n",
    "\n",
    "\n",
    "print(\"We recommend to read this paper............\")\n",
    "print(\"=============================================\")\n",
    "for paper in recommend_papers:\n",
    "    print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c696a388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the title of any paper you likeBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "We recommend to read this paper............\n",
      "=============================================\n",
      "BEiT: BERT Pre-Training of Image Transformers\n",
      "VL-BERT: Pre-training of Generic Visual-Linguistic Representations\n",
      "Sketch-BERT: Learning Sketch Bidirectional Encoder Representation from Transformers by Self-supervised Learning of Sketch Gestalt\n",
      "Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning\n",
      "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping\n"
     ]
    }
   ],
   "source": [
    "# exampel usage 2: (use this paper as input (BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding))\n",
    "input_paper = input(\"Enter the title of any paper you like\")\n",
    "recommend_papers = recommendation(input_paper)\n",
    "\n",
    "\n",
    "print(\"We recommend to read this paper............\")\n",
    "print(\"=============================================\")\n",
    "for paper in recommend_papers:\n",
    "    print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d6ee079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the title of any paper you likeReview of deep learning: concepts, CNN architectures, challenges, applications, future directions\n",
      "We recommend to read this paper............\n",
      "=============================================\n",
      "A Review of Deep Learning with Special Emphasis on Architectures, Applications and Recent Trends\n",
      "Review of Deep Learning\n",
      "Deep Convolutional Neural Networks: A survey of the foundations, selected improvements, and some current applications\n",
      "A Survey of the Recent Architectures of Deep Convolutional Neural Networks\n",
      "A Survey of Convolutional Neural Networks: Analysis, Applications, and Prospects\n"
     ]
    }
   ],
   "source": [
    "# exampel usage 3: (use this paper as input (Review of deep learning: concepts, CNN architectures, challenges, applications, future directions))\n",
    "input_paper = input(\"Enter the title of any paper you like\")\n",
    "recommend_papers = recommendation(input_paper)\n",
    "\n",
    "\n",
    "print(\"We recommend to read this paper............\")\n",
    "print(\"=============================================\")\n",
    "for paper in recommend_papers:\n",
    "    print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27d37d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cpu\n",
      "2.5.1\n",
      "2.14.0\n"
     ]
    }
   ],
   "source": [
    "# install this versions\n",
    "import sentence_transformers\n",
    "import tensorflow\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(sentence_transformers.__version__)\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebfbb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
